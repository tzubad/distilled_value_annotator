{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31347999",
   "metadata": {},
   "source": [
    "# üß™ Manual Testing: Evaluation Module\n",
    "\n",
    "This notebook allows you to manually test the evaluation module:\n",
    "1. **Load ground truth** from CSV\n",
    "2. **Create mock predictions** with different accuracy levels\n",
    "3. **Calculate metrics** (precision, recall, F1)\n",
    "4. **Generate reports** (CSV, JSON, comparison)\n",
    "5. **Test cloud pipeline** with GCS (optional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f61b3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Evaluation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071caa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import evaluation module components\n",
    "from evaluation.models import (\n",
    "    VideoAnnotation,\n",
    "    PredictionResult,\n",
    "    PredictionSet,\n",
    "    GroundTruthDataset,\n",
    "    EvaluationConfig,\n",
    "    ModelConfig,\n",
    ")\n",
    "from evaluation.ground_truth_loader import GroundTruthLoader, ANNOTATION_CATEGORIES\n",
    "from evaluation.metrics.calculator import MetricsCalculator, ModelEvaluationResult\n",
    "from evaluation.reports.generator import ReportGenerator\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Annotation categories: {len(ANNOTATION_CATEGORIES)}\")\n",
    "print(f\"Categories: {ANNOTATION_CATEGORIES[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510962a",
   "metadata": {},
   "source": [
    "## 2. Load Ground Truth Data\n",
    "\n",
    "Load the ground truth dataset from your CSV file. This contains the actual annotations for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641560d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - adjust these paths as needed\n",
    "GROUND_TRUTH_PATH = \"cleaned_groundtruth_values_only.csv\"  # Your ground truth file\n",
    "SAMPLE_SIZE = None  # Set to a number (e.g., 50) for quick testing, None for all data\n",
    "OUTPUT_DIR = \"./manual_test_results\"\n",
    "\n",
    "# Load ground truth\n",
    "loader = GroundTruthLoader(\n",
    "    dataset_path=GROUND_TRUTH_PATH,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    random_seed=42,\n",
    ")\n",
    "ground_truth = loader.load()\n",
    "\n",
    "print(f\"‚úì Ground truth loaded!\")\n",
    "print(f\"  Total videos: {ground_truth.total_count}\")\n",
    "print(f\"  Valid videos: {ground_truth.valid_count}\")\n",
    "print(f\"  Validation errors: {len(ground_truth.validation_errors)}\")\n",
    "\n",
    "if ground_truth.validation_errors:\n",
    "    print(f\"\\n‚ö†Ô∏è First 3 validation errors:\")\n",
    "    for error in ground_truth.validation_errors[:3]:\n",
    "        print(f\"  - {error[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the ground truth data\n",
    "print(\"üìä Sample of loaded videos:\\n\")\n",
    "\n",
    "for i, video in enumerate(ground_truth.videos[:5]):\n",
    "    endorsed = sum(1 for v in video.annotations.values() if v in {1, 2})\n",
    "    conflict = sum(1 for v in video.annotations.values() if v == -1)\n",
    "    absent = sum(1 for v in video.annotations.values() if v == 0)\n",
    "    \n",
    "    print(f\"{i+1}. Video ID: {video.video_id}\")\n",
    "    print(f\"   Endorsed: {endorsed}, Conflict: {conflict}, Absent: {absent}\")\n",
    "    print(f\"   Sample annotations: {dict(list(video.annotations.items())[:3])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381adb0",
   "metadata": {},
   "source": [
    "## 3. Create Mock Predictions\n",
    "\n",
    "Create predictions with different accuracy levels to test the metrics calculation. You can also load your own predictions from a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_predictions(\n",
    "    ground_truth: GroundTruthDataset,\n",
    "    model_name: str = \"mock_model\",\n",
    "    accuracy_rate: float = 0.7,\n",
    "    failure_rate: float = 0.05,\n",
    "    seed: int = 42,\n",
    ") -> PredictionSet:\n",
    "    \"\"\"Create mock predictions with configurable accuracy.\"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    predictions = []\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    failed_ids = []\n",
    "    \n",
    "    for video in ground_truth.videos:\n",
    "        # Simulate occasional failures\n",
    "        if random.random() < failure_rate:\n",
    "            predictions.append(PredictionResult(\n",
    "                video_id=video.video_id,\n",
    "                predictions={},\n",
    "                success=False,\n",
    "                error_message=\"Simulated prediction failure\",\n",
    "                inference_time=0.0,\n",
    "            ))\n",
    "            failure_count += 1\n",
    "            failed_ids.append(video.video_id)\n",
    "            continue\n",
    "        \n",
    "        # Create predictions with some noise\n",
    "        pred_annotations = {}\n",
    "        for category, gt_value in video.annotations.items():\n",
    "            if random.random() < accuracy_rate:\n",
    "                pred_annotations[category] = gt_value  # Correct\n",
    "            else:\n",
    "                possible = [v for v in [-1, 0, 1, 2] if v != gt_value]\n",
    "                pred_annotations[category] = random.choice(possible)  # Wrong\n",
    "        \n",
    "        predictions.append(PredictionResult(\n",
    "            video_id=video.video_id,\n",
    "            predictions=pred_annotations,\n",
    "            success=True,\n",
    "            error_message=None,\n",
    "            inference_time=random.uniform(0.1, 2.0),\n",
    "        ))\n",
    "        success_count += 1\n",
    "    \n",
    "    return PredictionSet(\n",
    "        model_name=model_name,\n",
    "        predictions=predictions,\n",
    "        total_count=len(predictions),\n",
    "        success_count=success_count,\n",
    "        failure_count=failure_count,\n",
    "        failed_video_ids=failed_ids,\n",
    "    )\n",
    "\n",
    "print(\"‚úì Mock prediction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock predictions with different accuracy levels\n",
    "predictions_dict = {}\n",
    "\n",
    "model_configs = [\n",
    "    (\"high_accuracy_model\", 0.85, 0.02),\n",
    "    (\"medium_accuracy_model\", 0.65, 0.05),\n",
    "    (\"low_accuracy_model\", 0.45, 0.10),\n",
    "]\n",
    "\n",
    "for model_name, accuracy, failure_rate in model_configs:\n",
    "    preds = create_mock_predictions(\n",
    "        ground_truth,\n",
    "        model_name=model_name,\n",
    "        accuracy_rate=accuracy,\n",
    "        failure_rate=failure_rate,\n",
    "    )\n",
    "    predictions_dict[model_name] = preds\n",
    "    \n",
    "    print(f\"üì¶ {model_name}:\")\n",
    "    print(f\"   Total: {preds.total_count}, Success: {preds.success_count}, Failed: {preds.failure_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3ef4b",
   "metadata": {},
   "source": [
    "### Option: Load Your Own Predictions\n",
    "\n",
    "If you have actual predictions, you can load them from a JSON file. The format should be:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"video_id\": \"7441889182883829025\",\n",
    "        \"predictions\": {\n",
    "            \"Self_Direction_Thought\": 0,\n",
    "            \"Self_Direction_Action\": 1,\n",
    "            ...\n",
    "        },\n",
    "        \"success\": true\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e40abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify to load your own predictions\n",
    "# YOUR_PREDICTIONS_FILE = \"your_predictions.json\"\n",
    "\n",
    "# if Path(YOUR_PREDICTIONS_FILE).exists():\n",
    "#     with open(YOUR_PREDICTIONS_FILE, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     \n",
    "#     predictions = []\n",
    "#     for item in data:\n",
    "#         pred = PredictionResult(\n",
    "#             video_id=item['video_id'],\n",
    "#             predictions=item['predictions'],\n",
    "#             success=item.get('success', True),\n",
    "#             error_message=item.get('error_message'),\n",
    "#             inference_time=item.get('inference_time', 0.0),\n",
    "#         )\n",
    "#         predictions.append(pred)\n",
    "#     \n",
    "#     your_predictions = PredictionSet(\n",
    "#         model_name=\"your_model\",\n",
    "#         predictions=predictions,\n",
    "#         total_count=len(predictions),\n",
    "#         success_count=sum(1 for p in predictions if p.success),\n",
    "#         failure_count=sum(1 for p in predictions if not p.success),\n",
    "#         failed_video_ids=[p.video_id for p in predictions if not p.success],\n",
    "#     )\n",
    "#     predictions_dict[\"your_model\"] = your_predictions\n",
    "#     print(f\"‚úì Loaded {len(predictions)} predictions from {YOUR_PREDICTIONS_FILE}\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Uncomment the code above to load your own predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf54130",
   "metadata": {},
   "source": [
    "## 4. Calculate Metrics\n",
    "\n",
    "Use the MetricsCalculator to compute precision, recall, F1 scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b82f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "results = {}\n",
    "\n",
    "for model_name, predictions in predictions_dict.items():\n",
    "    print(f\"\\nüîç Calculating metrics for {model_name}...\")\n",
    "    \n",
    "    calculator = MetricsCalculator(\n",
    "        ground_truth=ground_truth,\n",
    "        min_frequency_threshold=0.05,  # Exclude rare categories (<5%)\n",
    "    )\n",
    "    \n",
    "    result = calculator.calculate_model_metrics(predictions)\n",
    "    results[model_name] = result\n",
    "    \n",
    "    print(f\"   ‚úì Matched videos: {result.matched_with_ground_truth}\")\n",
    "    print(f\"   ‚úì Unmatched predictions: {result.unmatched_count}\")\n",
    "    print(f\"   ‚úì Missing predictions: {result.missing_count}\")\n",
    "\n",
    "print(f\"\\n‚úì Calculated metrics for {len(results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc6162",
   "metadata": {},
   "source": [
    "## 5. Validate and Inspect Metric Results\n",
    "\n",
    "View the aggregate and per-category metrics to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660296e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare aggregate metrics across models\n",
    "print(\"üìä AGGREGATE METRICS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<25} {'Endorsed F1':>12} {'Conflict F1':>12} {'Combined F1':>12} {'Categories':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name:<25} \"\n",
    "          f\"{result.endorsed_aggregate.macro_f1:>12.4f} \"\n",
    "          f\"{result.conflict_aggregate.macro_f1:>12.4f} \"\n",
    "          f\"{result.combined_aggregate.macro_f1:>12.4f} \"\n",
    "          f\"{result.endorsed_aggregate.categories_evaluated:>12}\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a885a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-category breakdown for the best model\n",
    "best_model = max(results.items(), key=lambda x: x[1].endorsed_aggregate.macro_f1)\n",
    "print(f\"\\nüìà PER-CATEGORY METRICS: {best_model[0]}\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Category':<30} {'Endorsed F1':>12} {'Conflict F1':>12} {'Support':>10} {'TP':>8} {'FP':>8} {'FN':>8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "result = best_model[1]\n",
    "for category in ANNOTATION_CATEGORIES:\n",
    "    endorsed = result.per_category_endorsed.get(category)\n",
    "    conflict = result.per_category_conflict.get(category)\n",
    "    if endorsed:\n",
    "        print(f\"{category:<30} \"\n",
    "              f\"{endorsed.f1:>12.4f} \"\n",
    "              f\"{conflict.f1 if conflict else 0:>12.4f} \"\n",
    "              f\"{endorsed.support:>10} \"\n",
    "              f\"{endorsed.true_positives:>8} \"\n",
    "              f\"{endorsed.false_positives:>8} \"\n",
    "              f\"{endorsed.false_negatives:>8}\")\n",
    "\n",
    "print(\"-\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70cafa8",
   "metadata": {},
   "source": [
    "## 6. Generate and Inspect Reports\n",
    "\n",
    "Generate CSV and JSON reports for the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Generate reports\n",
    "generator = ReportGenerator(OUTPUT_DIR)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "generated_files = generator.generate_all_reports(list(results.values()), timestamp)\n",
    "\n",
    "print(\"üìÑ GENERATED REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "for report_type, path in generated_files.items():\n",
    "    file_size = os.path.getsize(path)\n",
    "    print(f\"  {report_type}: {path.name} ({file_size} bytes)\")\n",
    "\n",
    "print(f\"\\n‚úì All reports saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf301f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the comparison report (if generated)\n",
    "comparison_file = [p for t, p in generated_files.items() if \"comparison\" in t]\n",
    "if comparison_file:\n",
    "    print(\"üìã COMPARISON REPORT PREVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    with open(comparison_file[0], 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:30]:  # First 30 lines\n",
    "            print(line.rstrip())\n",
    "    if len(lines) > 30:\n",
    "        print(f\"... ({len(lines) - 30} more lines)\")\n",
    "else:\n",
    "    # Preview a single model's JSON report\n",
    "    json_files = [p for t, p in generated_files.items() if \"json\" in t]\n",
    "    if json_files:\n",
    "        print(\"üìã JSON REPORT PREVIEW (first model)\")\n",
    "        print(\"=\" * 60)\n",
    "        with open(json_files[0], 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(json.dumps(data[\"summary\"], indent=2))\n",
    "        print(json.dumps(data[\"aggregate_metrics\"][\"endorsed\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15959e86",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test Cloud Pipeline (Optional)\n",
    "\n",
    "Test loading scripts from Google Cloud Storage. Make sure you have:\n",
    "1. Authenticated with GCloud: `gcloud auth application-default login`\n",
    "2. Scripts uploaded to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud configuration\n",
    "BUCKET_NAME = \"videos-scripts-and-annotations\"  # Your GCS bucket\n",
    "SCRIPTS_PREFIX = \"saved_scripts/POC_scripts\"  # Path to scripts in bucket\n",
    "\n",
    "# Try to import GCS client\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    GCS_AVAILABLE = True\n",
    "    print(\"‚úì Google Cloud Storage library available\")\n",
    "except ImportError:\n",
    "    GCS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è google-cloud-storage not installed. Run: pip install google-cloud-storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List scripts in GCS bucket\n",
    "if GCS_AVAILABLE:\n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(BUCKET_NAME)\n",
    "        \n",
    "        print(f\"üìÇ Listing scripts in gs://{BUCKET_NAME}/{SCRIPTS_PREFIX}/\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        blobs = list(bucket.list_blobs(prefix=SCRIPTS_PREFIX, max_results=10))\n",
    "        \n",
    "        if blobs:\n",
    "            for blob in blobs:\n",
    "                print(f\"  {blob.name} ({blob.size} bytes)\")\n",
    "            print(f\"\\n‚úì Found {len(blobs)}+ scripts\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è No scripts found in this location\")\n",
    "            print(\"  Make sure you've uploaded scripts to the bucket\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to GCS: {e}\")\n",
    "        print(\"\\nTry running: gcloud auth application-default login\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GCS test - library not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test script loading through the ScriptLoader\n",
    "if GCS_AVAILABLE:\n",
    "    from evaluation.adapters.script_loader import ScriptLoader\n",
    "    \n",
    "    script_loader = ScriptLoader()\n",
    "    \n",
    "    print(\"üîÑ Testing script loading from GCS...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Test with a few videos from ground truth\n",
    "    test_videos = ground_truth.videos[:3]\n",
    "    \n",
    "    for video in test_videos:\n",
    "        # Construct GCS URI for the script\n",
    "        script_uri = f\"gs://{BUCKET_NAME}/{SCRIPTS_PREFIX}/{video.video_id}.txt\"\n",
    "        \n",
    "        content = script_loader.load_script(script_uri)\n",
    "        \n",
    "        if content:\n",
    "            preview = content[:100].replace('\\n', ' ')\n",
    "            print(f\"‚úì {video.video_id}: {len(content)} chars\")\n",
    "            print(f\"  Preview: {preview}...\")\n",
    "        else:\n",
    "            print(f\"‚úó {video.video_id}: Script not found\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Cache size: {script_loader.get_cache_size()} scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c612a6",
   "metadata": {},
   "source": [
    "### Upload Local Scripts to GCS (Optional)\n",
    "\n",
    "If you have local scripts you want to test with, run this cell to upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to upload local scripts\n",
    "# LOCAL_SCRIPTS_DIR = \"./my_local_scripts\"  # Change to your scripts directory\n",
    "\n",
    "# if GCS_AVAILABLE and Path(LOCAL_SCRIPTS_DIR).exists():\n",
    "#     client = storage.Client()\n",
    "#     bucket = client.bucket(BUCKET_NAME)\n",
    "#     \n",
    "#     script_files = list(Path(LOCAL_SCRIPTS_DIR).glob(\"*.txt\"))\n",
    "#     print(f\"üì§ Uploading {len(script_files)} scripts to GCS...\")\n",
    "#     \n",
    "#     for script_file in script_files:\n",
    "#         blob_path = f\"{SCRIPTS_PREFIX}/{script_file.name}\"\n",
    "#         blob = bucket.blob(blob_path)\n",
    "#         blob.upload_from_filename(str(script_file))\n",
    "#         print(f\"  ‚úì Uploaded: {script_file.name}\")\n",
    "#     \n",
    "#     print(f\"\\n‚úì All scripts uploaded to gs://{BUCKET_NAME}/{SCRIPTS_PREFIX}/\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Uncomment the code above to upload local scripts to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccbe14",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook tested the evaluation module with:\n",
    "- ‚úÖ Ground truth loading from CSV\n",
    "- ‚úÖ Mock predictions with different accuracy levels\n",
    "- ‚úÖ Metrics calculation (precision, recall, F1)\n",
    "- ‚úÖ Report generation (CSV, JSON, comparison)\n",
    "- ‚úÖ Cloud pipeline testing (optional)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Replace mock predictions with your actual model predictions\n",
    "2. Upload your scripts to GCS and verify they load correctly\n",
    "3. Run the full evaluation pipeline with `python run_evaluation.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
